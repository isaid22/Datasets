{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b834ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, math, json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e09eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss function: MSELoss()\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Load config\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "csv_path       = cfg[\"csv_file\"]\n",
    "NUMERIC        = cfg[\"numeric_features\"]\n",
    "CATEG          = cfg[\"categorical_features\"]\n",
    "target_col     = cfg[\"target\"]\n",
    "task_type      = cfg[\"task_type\"]\n",
    "loss_name      = cfg[\"loss_function\"]\n",
    "optimizer_cfg  = cfg[\"optimizer\"]\n",
    "\n",
    "# 2. Choose loss function dynamically\n",
    "if loss_name == \"MSELoss\":\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif loss_name == \"CrossEntropyLoss\":\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported loss function: {loss_name}\")\n",
    "\n",
    "print(\"Using loss function:\", loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440db8d6",
   "metadata": {},
   "source": [
    "## Fit statistics and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b17382aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Purchase Amount (USD)', 'Review Rating', 'Previous Purchases']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NUMERIC = [\"Age\", \"Purchase Amount (USD)\", \"Review Rating\", \"Previous Purchases\"]\n",
    "# CATEG = [\"Gender\",\"Item Purchased\",\"Category\",\"Location\",\"Size\",\"Color\",\"Season\",\n",
    "#          \"Subscription Status\",\"Shipping Type\",\"Discount Applied\",\"Promo Code Used\",\n",
    "#          \"Payment Method\",\"Frequency of Purchases\"]\n",
    "\n",
    "print(NUMERIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7956ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welford’s online mean/std\n",
    "class OnlineScaler:\n",
    "    def __init__(self): self.n=0; self.mean=0.0; self.M2=0.0\n",
    "    def update(self, x):\n",
    "        self.n += 1\n",
    "        d = x - self.mean\n",
    "        self.mean += d / self.n\n",
    "        self.M2 += d * (x - self.mean)\n",
    "    def stats(self):\n",
    "        var = self.M2 / (self.n-1) if self.n>1 else 1.0\n",
    "        std = math.sqrt(var) if var>0 else 1.0\n",
    "        return {\"mean\": self.mean, \"std\": std}\n",
    "\n",
    "def fit_transformer(csv_path, encoding=\"utf-8\"):\n",
    "    scalers = {col: OnlineScaler() for col in NUMERIC}\n",
    "    vocab = {col: set() for col in CATEG}\n",
    "\n",
    "    with open(csv_path, newline=\"\", encoding=encoding) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            # numeric\n",
    "            for col in NUMERIC:\n",
    "                v = row.get(col, \"\")\n",
    "                if v != \"\":\n",
    "                    try: scalers[col].update(float(v))\n",
    "                    except: pass\n",
    "            # categorical\n",
    "            for col in CATEG:\n",
    "                v = (row.get(col, \"\") or \"\").strip() or \"<UNK>\"\n",
    "                vocab[col].add(v)\n",
    "\n",
    "    # freeze vocab -> index (reserve 0 for <UNK>)\n",
    "    cat2idx = {}\n",
    "    for col in CATEG:\n",
    "        items = sorted(vocab[col] - {\"<UNK>\"})\n",
    "        cat2idx[col] = {\"<UNK>\": 0}\n",
    "        for i, val in enumerate(items, start=1):\n",
    "            cat2idx[col][val] = i\n",
    "\n",
    "    # freeze scalers\n",
    "    scaler_stats = {col: scalers[col].stats() for col in NUMERIC}\n",
    "\n",
    "    return {\"numeric\": scaler_stats, \"categorical\": cat2idx}\n",
    "\n",
    "# Example:\n",
    "# tf = fit_transformer(\"shopping_benavior_updated.csv\")\n",
    "# json.dump(tf, open(\"transform.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465b5c9",
   "metadata": {},
   "source": [
    "## Stream rows and tensors with an `IterableDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49b51be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "import csv\n",
    "\n",
    "def make_row_transform(transform_frozen, target):\n",
    "    num_stats = transform_frozen[\"numeric\"]\n",
    "    cat_map = transform_frozen[\"categorical\"]\n",
    "\n",
    "    def to_example(row):\n",
    "        # numeric -> z-score\n",
    "        x_num = []\n",
    "        for col in NUMERIC:\n",
    "            v = row.get(col, \"\")\n",
    "            if v == \"\":\n",
    "                m, s = num_stats[col][\"mean\"], num_stats[col][\"std\"]\n",
    "                x_num.append((0.0 - m) / s)  # or use m as fill\n",
    "            else:\n",
    "                val = float(v)\n",
    "                m, s = num_stats[col][\"mean\"], num_stats[col][\"std\"]\n",
    "                x_num.append((val - m) / s)\n",
    "\n",
    "        # categorical -> index\n",
    "        x_cat = []\n",
    "        for col in CATEG:\n",
    "            val = (row.get(col, \"\") or \"\").strip()\n",
    "            idx = cat_map[col].get(val, 0)  # 0 = <UNK>\n",
    "            x_cat.append(idx)\n",
    "\n",
    "        # target (regression on Purchase Amount)\n",
    "        y = float(row[target])\n",
    "\n",
    "        x_num = torch.tensor(x_num, dtype=torch.float32)\n",
    "        x_cat = torch.tensor(x_cat, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return (x_num, x_cat), y\n",
    "\n",
    "    return to_example\n",
    "\n",
    "class StreamingCSVDataset(IterableDataset):\n",
    "    def __init__(self, csv_path, row_transform, encoding=\"utf-8\"):\n",
    "        self.csv_path = csv_path\n",
    "        self.row_transform = row_transform\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.csv_path, newline=\"\", encoding=self.encoding) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                yield self.row_transform(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607837b0",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f8a7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tf = fit_transformer(csv_path)\n",
    "json.dump(tf, open(\"transform.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acadb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load & build dataset/loader\n",
    "tf = json.load(open(\"transform.json\"))\n",
    "row_tf = make_row_transform(tf, target=\"Purchase Amount (USD)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2059445",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StreamingCSVDataset(\"shopping_behavior_updated.csv\", row_tf)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b856467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_num: torch.Size([32, 4])\n",
      "X_cat: torch.Size([32, 13])\n",
      "y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 3) Look at one batch\n",
    "(X_num, X_cat), y = next(iter(loader))\n",
    "print(\"X_num:\", X_num.shape)   # [B, 4]\n",
    "print(\"X_cat:\", X_cat.shape)   # [B, 13]\n",
    "print(\"y:\", y.shape)           # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b127085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3638.48974609375\n"
     ]
    }
   ],
   "source": [
    "# 4) Simple model skeleton (numerical + embeddings)\n",
    "import torch.nn as nn\n",
    "\n",
    "# build embedding sizes from vocab\n",
    "embeddings = nn.ModuleList()\n",
    "cat_dims = [len(tf[\"categorical\"][c]) for c in CATEG]\n",
    "emb_dims = [min(50, (d//2)+1) for d in cat_dims]  # rule-of-thumb\n",
    "\n",
    "for d, e in zip(cat_dims, emb_dims):\n",
    "    embeddings.append(nn.Embedding(num_embeddings=d, embedding_dim=e))\n",
    "\n",
    "class TabModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embs = embeddings\n",
    "        in_num = len(NUMERIC)\n",
    "        in_cat = sum(emb_dims)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_num + in_cat, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [emb(x_cat[:,i]) for i, emb in enumerate(self.embs)]\n",
    "        x = torch.cat([x_num] + embs, dim=1)\n",
    "        return self.mlp(x).squeeze(-1)\n",
    "\n",
    "model = TabModel()\n",
    "crit = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 5) One tiny training step (just to show usage)\n",
    "(X_num, X_cat), y = next(iter(loader))\n",
    "pred = model(X_num, X_cat)\n",
    "loss = crit(pred, y)\n",
    "opt.zero_grad(); loss.backward(); opt.step()\n",
    "print(\"loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0faee",
   "metadata": {},
   "source": [
    "## Split dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4b904cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class StreamingCSVDataset(IterableDataset):\n",
    "    def __init__(self, csv_path, row_transform, split=\"train\", ratio=0.8):\n",
    "        self.csv_path = csv_path\n",
    "        self.row_transform = row_transform\n",
    "        self.split = split\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __iter__(self):\n",
    "        import csv\n",
    "        with open(self.csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                r = random.random()\n",
    "                if self.split == \"train\" and r < self.ratio:\n",
    "                    yield self.row_transform(row)\n",
    "                elif self.split == \"test\" and r >= self.ratio:\n",
    "                    yield self.row_transform(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82450dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = StreamingCSVDataset(\"shopping_behavior_updated.csv\", row_tf, split=\"train\", ratio=0.8)\n",
    "test_ds = StreamingCSVDataset(\"shopping_behavior_updated.csv\", row_tf, split=\"test\", ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64124673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([-1.6484,  0.1788, -0.9075, -1.6163]), tensor([ 2, 24,  2, 19,  1, 13,  4,  2,  2,  2,  2,  2,  4])), tensor(64.))\n",
      "((tensor([ 0.3900,  0.5588, -0.9075, -0.1628]), tensor([ 2, 12,  2, 21,  3, 13,  2,  2,  3,  2,  2,  3,  7])), tensor(73.))\n",
      "((tensor([-1.5169,  1.2766, -0.3490,  1.6369]), tensor([ 2, 15,  3, 39,  2, 13,  2,  2,  4,  2,  2,  5,  7])), tensor(90.))\n",
      "((tensor([ 0.0613, -0.4545, -1.4660,  0.3910]), tensor([ 2,  3,  2, 37,  2, 22,  2,  2,  3,  2,  2,  5,  1])), tensor(49.))\n",
      "((tensor([ 1.2449,  1.0655, -0.7678,  1.6369]), tensor([ 2, 17,  2, 26,  2,  8,  1,  2,  3,  2,  2,  2,  6])), tensor(85.))\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train_ds):\n",
    "    print(sample)\n",
    "    if i >= 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8539f4",
   "metadata": {},
   "source": [
    "## Welford's algorithm\n",
    "Welford’s online algorithm is purely a numerical method for computing the mean and variance incrementally (in one streaming pass), without keeping the entire dataset in memory. Mathematically, it computes mean and variance.\n",
    "The “online” version just updates these quantities safely and stably with each new observation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recmd_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
